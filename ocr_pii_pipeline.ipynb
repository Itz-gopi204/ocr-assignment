{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Pipeline for Handwritten Document PII Extraction\n",
    "\n",
    "This notebook implements an end-to-end pipeline for:\n",
    "1. **Pre-processing** - Image enhancement (deskew, denoise, binarization)\n",
    "2. **OCR** - Text extraction from handwritten documents\n",
    "3. **Text Cleaning** - Post-processing of extracted text\n",
    "4. **PII Detection** - Identifying personal identifiable information\n",
    "5. **Image Redaction** (Optional) - Masking PII in the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install opencv-python numpy pillow pytesseract easyocr spacy regex\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import pytesseract\n",
    "import easyocr\n",
    "import re\n",
    "import spacy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load spaCy model for NER\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass PipelineConfig:\n    \"\"\"Configuration for the OCR Pipeline - Optimized for handwritten medical documents\"\"\"\n    # Pre-processing settings\n    resize_width: int = 2500  # Larger for better OCR\n    denoise_strength: int = 8  # Reduced to preserve handwriting details\n\n    # OCR settings\n    ocr_engine: str = \"easyocr\"  # Best for handwriting\n    tesseract_config: str = \"--oem 3 --psm 6\"\n    easyocr_languages: List[str] = None\n    min_confidence: float = 0.25  # Lower threshold to capture more text\n\n    # PII Detection settings\n    detect_names: bool = True\n    detect_dates: bool = True\n    detect_phone: bool = True\n    detect_ids: bool = True\n    detect_age: bool = True\n    detect_address: bool = True\n\n    def __post_init__(self):\n        if self.easyocr_languages is None:\n            self.easyocr_languages = [\"en\"]\n\n# Default configuration\nconfig = PipelineConfig()\nprint(f\"Pipeline configured with OCR engine: {config.ocr_engine}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Pre-processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ImagePreprocessor:\n    \"\"\"\n    Enhanced image pre-processing optimized for:\n    - Slightly tilted images (advanced deskew)\n    - Different handwriting styles\n    - Doctor/clinic-style notes and forms\n    \"\"\"\n\n    def __init__(self, config: PipelineConfig):\n        self.config = config\n\n    def load_image(self, image_path: str) -> np.ndarray:\n        \"\"\"Load image from file path\"\"\"\n        image = cv2.imread(image_path)\n        if image is None:\n            raise ValueError(f\"Could not load image from {image_path}\")\n        return image\n\n    def resize_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Resize image while maintaining aspect ratio\"\"\"\n        height, width = image.shape[:2]\n        if width > self.config.resize_width:\n            ratio = self.config.resize_width / width\n            new_height = int(height * ratio)\n            image = cv2.resize(image, (self.config.resize_width, new_height),\n                             interpolation=cv2.INTER_LANCZOS4)\n        return image\n\n    def convert_to_grayscale(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Convert image to grayscale\"\"\"\n        if len(image.shape) == 3:\n            return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        return image\n\n    def remove_shadows(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Remove shadows from image - important for phone camera photos\"\"\"\n        dilated = cv2.dilate(image, np.ones((7, 7), np.uint8))\n        bg = cv2.medianBlur(dilated, 21)\n        diff = 255 - cv2.absdiff(image, bg)\n        normalized = cv2.normalize(diff, None, 0, 255, cv2.NORM_MINMAX)\n        return normalized\n\n    def deskew_hough(self, image: np.ndarray) -> Tuple[np.ndarray, float]:\n        \"\"\"Deskew using Hough Line Transform - good for documents with lines\"\"\"\n        edges = cv2.Canny(image, 50, 150, apertureSize=3)\n        lines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n\n        if lines is not None:\n            angles = []\n            for line in lines:\n                rho, theta = line[0]\n                angle = (theta * 180 / np.pi) - 90\n                if -45 < angle < 45:\n                    angles.append(angle)\n\n            if angles:\n                median_angle = np.median(angles)\n                if abs(median_angle) > 0.5:\n                    return self._rotate_image(image, median_angle), median_angle\n        return image, 0.0\n\n    def deskew_minarea(self, image: np.ndarray) -> Tuple[np.ndarray, float]:\n        \"\"\"Deskew using minimum area rectangle - good for handwritten text\"\"\"\n        _, binary = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n        contours, _ = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n        if not contours:\n            return image, 0.0\n\n        all_points = np.vstack(contours)\n        rect = cv2.minAreaRect(all_points)\n        angle = rect[-1]\n\n        if angle < -45:\n            angle = 90 + angle\n        elif angle > 45:\n            angle = angle - 90\n\n        if abs(angle) > 0.5:\n            return self._rotate_image(image, angle), angle\n        return image, 0.0\n\n    def _rotate_image(self, image: np.ndarray, angle: float) -> np.ndarray:\n        \"\"\"Rotate image by given angle\"\"\"\n        if abs(angle) < 0.5:\n            return image\n        (h, w) = image.shape[:2]\n        center = (w // 2, h // 2)\n        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n        rotated = cv2.warpAffine(image, M, (w, h),\n                                 flags=cv2.INTER_CUBIC,\n                                 borderMode=cv2.BORDER_REPLICATE)\n        return rotated\n\n    def advanced_deskew(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Advanced deskew combining multiple methods for tilted images\"\"\"\n        best_result = image\n        best_angle = 0\n\n        # Method 1: Hough Transform\n        try:\n            result1, angle1 = self.deskew_hough(image.copy())\n            if 0.5 < abs(angle1) < 30:\n                best_result = result1\n                best_angle = angle1\n        except Exception:\n            pass\n\n        # Method 2: MinArea Rectangle\n        if abs(best_angle) < 1:\n            try:\n                result2, angle2 = self.deskew_minarea(image.copy())\n                if abs(angle2) > abs(best_angle) and abs(angle2) < 30:\n                    best_result = result2\n            except Exception:\n                pass\n\n        return best_result\n\n    def enhance_contrast(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Enhance image contrast using CLAHE\"\"\"\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        return clahe.apply(image)\n\n    def denoise(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Remove noise from image\"\"\"\n        return cv2.fastNlMeansDenoising(image, None,\n                                        self.config.denoise_strength, 7, 21)\n\n    def enhance_handwriting(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Special enhancement for different handwriting styles\"\"\"\n        # Bilateral filter preserves edges while removing noise\n        filtered = cv2.bilateralFilter(image, 9, 75, 75)\n\n        # Sharpen to make handwriting clearer\n        kernel = np.array([[-1,-1,-1],\n                          [-1, 9,-1],\n                          [-1,-1,-1]])\n        sharpened = cv2.filter2D(filtered, -1, kernel)\n\n        return cv2.normalize(sharpened, None, 0, 255, cv2.NORM_MINMAX)\n\n    def preprocess(self, image_path: str,\n                   apply_deskew: bool = True,\n                   apply_denoise: bool = True,\n                   apply_threshold: bool = False) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Full pre-processing pipeline optimized for:\n        - Slightly tilted images\n        - Different handwriting styles\n        - Medical document forms\n        \"\"\"\n        # Load and resize\n        original = self.load_image(image_path)\n        image = self.resize_image(original.copy())\n\n        # Convert to grayscale\n        gray = self.convert_to_grayscale(image)\n\n        # Remove shadows (important for phone photos)\n        gray = self.remove_shadows(gray)\n\n        # Deskew tilted images\n        if apply_deskew:\n            gray = self.advanced_deskew(gray)\n\n        # Enhance contrast\n        gray = self.enhance_contrast(gray)\n\n        # Enhance for handwriting\n        gray = self.enhance_handwriting(gray)\n\n        # Denoise\n        if apply_denoise:\n            gray = self.denoise(gray)\n\n        return original, gray\n\n# Initialize preprocessor\npreprocessor = ImagePreprocessor(config)\nprint(\"Image preprocessor initialized!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OCR Engine Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCREngine:\n",
    "    \"\"\"Handles OCR text extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.easyocr_reader = None\n",
    "        \n",
    "        # Initialize EasyOCR reader (lazy loading)\n",
    "        if config.ocr_engine in [\"easyocr\", \"both\"]:\n",
    "            print(\"Initializing EasyOCR... (this may take a moment)\")\n",
    "            self.easyocr_reader = easyocr.Reader(config.easyocr_languages, gpu=False)\n",
    "            print(\"EasyOCR initialized!\")\n",
    "    \n",
    "    def extract_with_tesseract(self, image: np.ndarray) -> Dict:\n",
    "        \"\"\"Extract text using Tesseract OCR\"\"\"\n",
    "        # Get detailed data\n",
    "        data = pytesseract.image_to_data(image, \n",
    "                                         config=self.config.tesseract_config,\n",
    "                                         output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        # Get plain text\n",
    "        text = pytesseract.image_to_string(image, \n",
    "                                           config=self.config.tesseract_config)\n",
    "        \n",
    "        # Extract bounding boxes\n",
    "        boxes = []\n",
    "        for i in range(len(data['text'])):\n",
    "            if int(data['conf'][i]) > 0:  # Filter low confidence\n",
    "                boxes.append({\n",
    "                    'text': data['text'][i],\n",
    "                    'confidence': data['conf'][i],\n",
    "                    'bbox': (data['left'][i], data['top'][i], \n",
    "                            data['width'][i], data['height'][i])\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'engine': 'tesseract',\n",
    "            'text': text,\n",
    "            'boxes': boxes\n",
    "        }\n",
    "    \n",
    "    def extract_with_easyocr(self, image: np.ndarray) -> Dict:\n",
    "        \"\"\"Extract text using EasyOCR\"\"\"\n",
    "        results = self.easyocr_reader.readtext(image)\n",
    "        \n",
    "        # Compile text\n",
    "        text_parts = []\n",
    "        boxes = []\n",
    "        \n",
    "        for (bbox, text, confidence) in results:\n",
    "            text_parts.append(text)\n",
    "            # Convert bbox format\n",
    "            x_coords = [p[0] for p in bbox]\n",
    "            y_coords = [p[1] for p in bbox]\n",
    "            boxes.append({\n",
    "                'text': text,\n",
    "                'confidence': confidence * 100,\n",
    "                'bbox': (int(min(x_coords)), int(min(y_coords)),\n",
    "                        int(max(x_coords) - min(x_coords)),\n",
    "                        int(max(y_coords) - min(y_coords))),\n",
    "                'polygon': bbox\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'engine': 'easyocr',\n",
    "            'text': '\\n'.join(text_parts),\n",
    "            'boxes': boxes\n",
    "        }\n",
    "    \n",
    "    def extract(self, image: np.ndarray) -> Dict:\n",
    "        \"\"\"Extract text using configured OCR engine(s)\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if self.config.ocr_engine == \"tesseract\":\n",
    "            results = self.extract_with_tesseract(image)\n",
    "        elif self.config.ocr_engine == \"easyocr\":\n",
    "            results = self.extract_with_easyocr(image)\n",
    "        elif self.config.ocr_engine == \"both\":\n",
    "            tesseract_result = self.extract_with_tesseract(image)\n",
    "            easyocr_result = self.extract_with_easyocr(image)\n",
    "            \n",
    "            # Combine results (prefer EasyOCR for handwriting)\n",
    "            results = {\n",
    "                'engine': 'combined',\n",
    "                'tesseract': tesseract_result,\n",
    "                'easyocr': easyocr_result,\n",
    "                'text': easyocr_result['text'],  # Primary text from EasyOCR\n",
    "                'boxes': easyocr_result['boxes']\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize OCR engine\n",
    "ocr_engine = OCREngine(config)\n",
    "print(\"OCR engine initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Cleaning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"Handles text post-processing and cleaning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common OCR errors in medical documents\n",
    "        self.corrections = {\n",
    "            '|': 'I',\n",
    "            '0': 'O',  # Context-dependent\n",
    "            '1': 'l',  # Context-dependent\n",
    "            '\\\\': '',\n",
    "            '`': \"'\",\n",
    "        }\n",
    "        \n",
    "        # Medical abbreviations to preserve\n",
    "        self.medical_abbrevs = [\n",
    "            'mg', 'ml', 'IV', 'IM', 'PO', 'BD', 'TID', 'QID', \n",
    "            'PRN', 'STAT', 'OD', 'BP', 'HR', 'RR', 'SPO2',\n",
    "            'Tab', 'Cap', 'Inj', 'Syp', 'Dr', 'IPD', 'OPD', 'UHID'\n",
    "        ]\n",
    "    \n",
    "    def remove_extra_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Remove extra whitespace while preserving structure\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # Replace multiple newlines with double newline\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def remove_noise_characters(self, text: str) -> str:\n",
    "        \"\"\"Remove common noise characters from OCR\"\"\"\n",
    "        # Remove isolated special characters\n",
    "        text = re.sub(r'(?<![\\w])[\\_\\-\\~\\^\\*\\#\\@]+(?![\\w])', '', text)\n",
    "        # Remove very short lines (likely noise)\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = [line for line in lines if len(line.strip()) > 1]\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def fix_common_ocr_errors(self, text: str) -> str:\n",
    "        \"\"\"Fix common OCR misrecognitions\"\"\"\n",
    "        # Fix common patterns\n",
    "        text = re.sub(r'\\bPaticnt\\b', 'Patient', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\bNamc\\b', 'Name', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\bAgc\\b', 'Age', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\bScx\\b', 'Sex', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\bDatc\\b', 'Date', text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "    \n",
    "    def normalize_dates(self, text: str) -> str:\n",
    "        \"\"\"Normalize date formats\"\"\"\n",
    "        # Common date patterns: DD/MM/YY, DD-MM-YYYY, etc.\n",
    "        # Keep original format but fix spacing\n",
    "        text = re.sub(r'(\\d{1,2})\\s*[/\\-\\.]\\s*(\\d{1,2})\\s*[/\\-\\.]\\s*(\\d{2,4})', \n",
    "                     r'\\1/\\2/\\3', text)\n",
    "        return text\n",
    "    \n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"Apply full text cleaning pipeline\"\"\"\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        text = self.remove_noise_characters(text)\n",
    "        text = self.fix_common_ocr_errors(text)\n",
    "        text = self.normalize_dates(text)\n",
    "        return text\n",
    "\n",
    "# Initialize text cleaner\n",
    "text_cleaner = TextCleaner()\n",
    "print(\"Text cleaner initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PII Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass PIIEntity:\n    \"\"\"Represents a detected PII entity\"\"\"\n    type: str\n    value: str\n    start: int\n    end: int\n    confidence: float\n\n    def to_dict(self):\n        return {\n            'type': self.type,\n            'value': self.value,\n            'start': self.start,\n            'end': self.end,\n            'confidence': self.confidence\n        }\n\n\nclass PIIDetector:\n    \"\"\"\n    PII Detection optimized for medical/hospital documents.\n\n    Detects:\n    - Patient Name\n    - Age\n    - Sex/Gender\n    - UHID (Unique Health ID)\n    - IPD No (Inpatient Department Number)\n    - Bed No\n    - Dates\n    - Doctor Names\n    - Phone/Mobile Numbers\n    - Registration Numbers\n    \"\"\"\n\n    def __init__(self, config: PipelineConfig):\n        self.config = config\n        self.nlp = nlp\n\n        # Medical document PII patterns\n        self.patterns = {\n            'PHONE': [\n                r'\\b(?:\\+91[\\-\\s]?)?[6-9]\\d{9}\\b',  # Indian mobile\n                r'\\b\\d{3}[\\-\\s]?\\d{3}[\\-\\s]?\\d{4}\\b',  # General format\n                r'\\b(?:Mobile|Phone|Tel|Contact)[:\\s]*([\\d\\-\\s\\+]+)\\b'\n            ],\n            'DATE': [\n                r'\\b\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4}\\b',  # DD/MM/YYYY\n                r'\\b\\d{1,2}[\\s\\-](?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\\s\\-]\\d{2,4}\\b',\n            ],\n            'ID_NUMBER': [\n                r'\\b(?:UHID|IPD|OPD|MRN|Reg)[\\s\\.\\-:No]*[:\\s]*([A-Z0-9\\-]+)\\b',\n                r'\\b(?:Bed)[\\s\\.\\-:No]*[:\\s]*(\\d+)\\b',\n            ],\n            'AGE': [\n                r'\\b(?:Age)[\\s:]*([\\d]+)[\\s]?(?:Y|yr|years?|yrs?)?\\b',\n                r'\\b(\\d{1,3})[\\s]?(?:Y|yr|years|yrs)\\b',\n            ],\n            'GENDER': [\n                r'\\b(?:Sex|Gender)[\\s:]*([MF]|Male|Female)\\b',\n            ],\n        }\n\n        # Name patterns for medical documents\n        self.name_patterns = [\n            r'(?:Patient\\s*Name|Pat\\.?\\s*Name)[:\\s]+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})',\n            r'(?:Dr\\.?|Doctor)\\s+([A-Z][a-z]+(?:\\s+[A-Z]\\.?\\s*[a-z]*)?)',\n            r'(?:Consultant|Resident|Physician)[:\\s]+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2})',\n        ]\n\n    def _is_noise(self, value: str) -> bool:\n        \"\"\"Filter out OCR noise\"\"\"\n        v = value.strip()\n        if len(v) < 2:\n            return True\n        # Filter values with too many special characters\n        alpha_count = sum(1 for c in v if c.isalnum())\n        if alpha_count < len(v) * 0.5:\n            return True\n        # Filter common noise patterns\n        if v in ['&', 'Mle', 'Jle', 'Bl', 'ug', 'cr', 'Ln', 'LU']:\n            return True\n        return False\n\n    def detect_with_regex(self, text: str) -> List[PIIEntity]:\n        \"\"\"Detect PII using regex patterns\"\"\"\n        entities = []\n\n        for pii_type, patterns in self.patterns.items():\n            for pattern in patterns:\n                for match in re.finditer(pattern, text, re.IGNORECASE):\n                    value = match.group(1) if match.lastindex else match.group(0)\n                    value = value.strip()\n                    if self._is_noise(value):\n                        continue\n                    entities.append(PIIEntity(\n                        type=pii_type,\n                        value=value,\n                        start=match.start(),\n                        end=match.end(),\n                        confidence=0.9\n                    ))\n\n        # Detect names\n        for pattern in self.name_patterns:\n            for match in re.finditer(pattern, text, re.IGNORECASE):\n                value = match.group(1) if match.lastindex else match.group(0)\n                if len(value.strip()) > 2 and not self._is_noise(value):\n                    entities.append(PIIEntity(\n                        type='NAME',\n                        value=value.strip(),\n                        start=match.start(),\n                        end=match.end(),\n                        confidence=0.85\n                    ))\n\n        return entities\n\n    def detect_with_ner(self, text: str) -> List[PIIEntity]:\n        \"\"\"Detect PII using spaCy NER\"\"\"\n        entities = []\n        doc = self.nlp(text)\n\n        type_mapping = {\n            'PERSON': 'NAME',\n            'DATE': 'DATE',\n            'GPE': 'LOCATION',\n            'LOC': 'LOCATION',\n            'ORG': 'ORGANIZATION',\n        }\n\n        for ent in doc.ents:\n            if ent.label_ in type_mapping:\n                if len(ent.text) > 2 and not self._is_noise(ent.text):\n                    entities.append(PIIEntity(\n                        type=type_mapping[ent.label_],\n                        value=ent.text,\n                        start=ent.start_char,\n                        end=ent.end_char,\n                        confidence=0.75\n                    ))\n\n        return entities\n\n    def merge_entities(self, entities: List[PIIEntity]) -> List[PIIEntity]:\n        \"\"\"Merge overlapping entities, preferring higher confidence\"\"\"\n        if not entities:\n            return []\n\n        sorted_entities = sorted(entities, key=lambda x: (x.start, -x.confidence))\n        merged = [sorted_entities[0]]\n\n        for entity in sorted_entities[1:]:\n            last = merged[-1]\n            if entity.start < last.end:\n                if entity.confidence > last.confidence:\n                    merged[-1] = entity\n            else:\n                merged.append(entity)\n\n        return merged\n\n    def detect(self, text: str) -> List[PIIEntity]:\n        \"\"\"Detect all PII in text\"\"\"\n        regex_entities = self.detect_with_regex(text)\n        ner_entities = self.detect_with_ner(text)\n\n        all_entities = regex_entities + ner_entities\n        merged_entities = self.merge_entities(all_entities)\n\n        # Remove duplicates\n        seen = set()\n        unique_entities = []\n        for entity in merged_entities:\n            key = (entity.type, entity.value.lower())\n            if key not in seen:\n                seen.add(key)\n                unique_entities.append(entity)\n\n        return unique_entities\n\n# Initialize PII detector\npii_detector = PIIDetector(config)\nprint(\"PII detector initialized!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Image Redaction Module (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRedactor:\n",
    "    \"\"\"Redacts PII from images\"\"\"\n",
    "    \n",
    "    def __init__(self, redaction_color=(0, 0, 0)):\n",
    "        self.redaction_color = redaction_color\n",
    "    \n",
    "    def find_text_bbox(self, ocr_boxes: List[Dict], pii_value: str) -> Optional[Tuple]:\n",
    "        \"\"\"Find bounding box for PII text in OCR results\"\"\"\n",
    "        pii_lower = pii_value.lower().strip()\n",
    "        \n",
    "        for box in ocr_boxes:\n",
    "            box_text = box['text'].lower().strip()\n",
    "            if pii_lower in box_text or box_text in pii_lower:\n",
    "                return box['bbox']\n",
    "        \n",
    "        # Try partial matching\n",
    "        pii_words = pii_lower.split()\n",
    "        for word in pii_words:\n",
    "            if len(word) > 2:  # Skip short words\n",
    "                for box in ocr_boxes:\n",
    "                    if word in box['text'].lower():\n",
    "                        return box['bbox']\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def redact_image(self, image: np.ndarray, \n",
    "                     ocr_result: Dict, \n",
    "                     pii_entities: List[PIIEntity]) -> np.ndarray:\n",
    "        \"\"\"Redact PII from image\"\"\"\n",
    "        # Convert to PIL for drawing\n",
    "        if len(image.shape) == 2:  # Grayscale\n",
    "            pil_image = Image.fromarray(image).convert('RGB')\n",
    "        else:\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        \n",
    "        ocr_boxes = ocr_result.get('boxes', [])\n",
    "        \n",
    "        for entity in pii_entities:\n",
    "            bbox = self.find_text_bbox(ocr_boxes, entity.value)\n",
    "            if bbox:\n",
    "                x, y, w, h = bbox\n",
    "                # Add padding\n",
    "                padding = 5\n",
    "                draw.rectangle(\n",
    "                    [x - padding, y - padding, x + w + padding, y + h + padding],\n",
    "                    fill=self.redaction_color\n",
    "                )\n",
    "        \n",
    "        # Convert back to OpenCV format\n",
    "        redacted = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "        return redacted\n",
    "\n",
    "# Initialize redactor\n",
    "redactor = ImageRedactor()\n",
    "print(\"Image redactor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRPIIPipeline:\n",
    "    \"\"\"Main pipeline orchestrating all modules\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig = None):\n",
    "        self.config = config or PipelineConfig()\n",
    "        self.preprocessor = ImagePreprocessor(self.config)\n",
    "        self.ocr_engine = OCREngine(self.config)\n",
    "        self.text_cleaner = TextCleaner()\n",
    "        self.pii_detector = PIIDetector(self.config)\n",
    "        self.redactor = ImageRedactor()\n",
    "    \n",
    "    def process(self, image_path: str, \n",
    "                generate_redacted: bool = True) -> Dict:\n",
    "        \"\"\"Process a single image through the pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {image_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results = {\n",
    "            'input_file': image_path,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'stages': {}\n",
    "        }\n",
    "        \n",
    "        # Stage 1: Pre-processing\n",
    "        print(\"\\n[1/5] Pre-processing image...\")\n",
    "        original, processed = self.preprocessor.preprocess(image_path)\n",
    "        results['stages']['preprocessing'] = {\n",
    "            'status': 'completed',\n",
    "            'original_shape': original.shape,\n",
    "            'processed_shape': processed.shape\n",
    "        }\n",
    "        print(f\"  - Original size: {original.shape}\")\n",
    "        print(f\"  - Processed size: {processed.shape}\")\n",
    "        \n",
    "        # Stage 2: OCR\n",
    "        print(\"\\n[2/5] Extracting text with OCR...\")\n",
    "        ocr_result = self.ocr_engine.extract(processed)\n",
    "        results['stages']['ocr'] = {\n",
    "            'status': 'completed',\n",
    "            'engine': ocr_result['engine'],\n",
    "            'text_length': len(ocr_result['text']),\n",
    "            'boxes_count': len(ocr_result.get('boxes', []))\n",
    "        }\n",
    "        print(f\"  - Engine: {ocr_result['engine']}\")\n",
    "        print(f\"  - Extracted {len(ocr_result['text'])} characters\")\n",
    "        print(f\"  - Found {len(ocr_result.get('boxes', []))} text regions\")\n",
    "        \n",
    "        # Stage 3: Text Cleaning\n",
    "        print(\"\\n[3/5] Cleaning extracted text...\")\n",
    "        raw_text = ocr_result['text']\n",
    "        cleaned_text = self.text_cleaner.clean(raw_text)\n",
    "        results['stages']['text_cleaning'] = {\n",
    "            'status': 'completed',\n",
    "            'raw_length': len(raw_text),\n",
    "            'cleaned_length': len(cleaned_text)\n",
    "        }\n",
    "        results['raw_text'] = raw_text\n",
    "        results['cleaned_text'] = cleaned_text\n",
    "        print(f\"  - Raw text length: {len(raw_text)}\")\n",
    "        print(f\"  - Cleaned text length: {len(cleaned_text)}\")\n",
    "        \n",
    "        # Stage 4: PII Detection\n",
    "        print(\"\\n[4/5] Detecting PII...\")\n",
    "        pii_entities = self.pii_detector.detect(cleaned_text)\n",
    "        results['stages']['pii_detection'] = {\n",
    "            'status': 'completed',\n",
    "            'entities_found': len(pii_entities)\n",
    "        }\n",
    "        results['pii_entities'] = [e.to_dict() for e in pii_entities]\n",
    "        print(f\"  - Found {len(pii_entities)} PII entities\")\n",
    "        \n",
    "        # Group PII by type\n",
    "        pii_by_type = {}\n",
    "        for entity in pii_entities:\n",
    "            if entity.type not in pii_by_type:\n",
    "                pii_by_type[entity.type] = []\n",
    "            pii_by_type[entity.type].append(entity.value)\n",
    "        results['pii_summary'] = pii_by_type\n",
    "        \n",
    "        for pii_type, values in pii_by_type.items():\n",
    "            print(f\"    - {pii_type}: {values}\")\n",
    "        \n",
    "        # Stage 5: Image Redaction (Optional)\n",
    "        if generate_redacted:\n",
    "            print(\"\\n[5/5] Generating redacted image...\")\n",
    "            redacted_image = self.redactor.redact_image(\n",
    "                processed, ocr_result, pii_entities\n",
    "            )\n",
    "            results['redacted_image'] = redacted_image\n",
    "            results['stages']['redaction'] = {'status': 'completed'}\n",
    "            print(\"  - Redacted image generated\")\n",
    "        else:\n",
    "            results['stages']['redaction'] = {'status': 'skipped'}\n",
    "        \n",
    "        # Store processed images for visualization\n",
    "        results['original_image'] = original\n",
    "        results['processed_image'] = processed\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Processing completed!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_batch(self, image_paths: List[str], \n",
    "                      generate_redacted: bool = True) -> List[Dict]:\n",
    "        \"\"\"Process multiple images\"\"\"\n",
    "        results = []\n",
    "        for path in image_paths:\n",
    "            try:\n",
    "                result = self.process(path, generate_redacted)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {str(e)}\")\n",
    "                results.append({\n",
    "                    'input_file': path,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        return results\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = OCRPIIPipeline(config)\n",
    "print(\"\\nOCR PII Pipeline initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\ndef visualize_results(result: Dict, figsize=(20, 10)):\n    \"\"\"Visualize pipeline results\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=figsize)\n\n    # Original image\n    axes[0].imshow(cv2.cvtColor(result['original_image'], cv2.COLOR_BGR2RGB))\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n\n    # Processed image\n    axes[1].imshow(result['processed_image'], cmap='gray')\n    axes[1].set_title('Processed Image')\n    axes[1].axis('off')\n\n    # Redacted image (if available)\n    if 'redacted_image' in result:\n        axes[2].imshow(cv2.cvtColor(result['redacted_image'], cv2.COLOR_BGR2RGB))\n        axes[2].set_title('Redacted Image')\n    else:\n        axes[2].text(0.5, 0.5, 'Redaction\\nSkipped',\n                    ha='center', va='center', fontsize=20)\n        axes[2].set_title('Redacted Image (Skipped)')\n    axes[2].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef print_pii_report(result: Dict):\n    \"\"\"Print detailed PII report\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PII DETECTION REPORT\")\n    print(\"=\"*60)\n    print(f\"File: {result['input_file']}\")\n    print(f\"Timestamp: {result['timestamp']}\")\n    print(\"-\"*60)\n\n    if 'pii_summary' in result:\n        print(\"\\nDetected PII Entities:\")\n        for pii_type, values in result['pii_summary'].items():\n            print(f\"\\n  [{pii_type}]\")\n            for value in values:\n                print(f\"    - {value}\")\n    else:\n        print(\"\\nNo PII entities detected.\")\n\n    print(\"\\n\" + \"=\"*60)\n\ndef save_results(result: Dict, output_dir: str):\n    \"\"\"Save results to files - clean JSON format for benchmarking\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    base_name = os.path.splitext(os.path.basename(result['input_file']))[0]\n\n    # Save processed image\n    cv2.imwrite(\n        os.path.join(output_dir, f\"{base_name}_processed.jpg\"),\n        result['processed_image']\n    )\n\n    # Save redacted image\n    if 'redacted_image' in result:\n        cv2.imwrite(\n            os.path.join(output_dir, f\"{base_name}_redacted.jpg\"),\n            result['redacted_image']\n        )\n\n    # Visualization\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(cv2.cvtColor(result['original_image'], cv2.COLOR_BGR2RGB))\n    axes[0].set_title('Original')\n    axes[0].axis('off')\n    axes[1].imshow(result['processed_image'], cmap='gray')\n    axes[1].set_title('Processed')\n    axes[1].axis('off')\n    if 'redacted_image' in result:\n        axes[2].imshow(cv2.cvtColor(result['redacted_image'], cv2.COLOR_BGR2RGB))\n        axes[2].set_title('Redacted')\n    axes[2].axis('off')\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, f\"{base_name}_visualization.png\"), dpi=150)\n    plt.close()\n\n    # Save clean JSON report\n    report = {\n        'input_file': result['input_file'],\n        'timestamp': result['timestamp'],\n        'cleaned_text': result['cleaned_text'],\n        'pii_entities': result['pii_entities'],\n        'pii_summary': result['pii_summary']\n    }\n\n    with open(os.path.join(output_dir, f\"{base_name}_report.json\"), 'w') as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"Results saved to: {output_dir}/{base_name}_*\")\n\nprint(\"Visualization utilities loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample image paths\n",
    "SAMPLES_DIR = \"samples\"\n",
    "OUTPUT_DIR = \"results\"\n",
    "\n",
    "sample_images = [\n",
    "    os.path.join(SAMPLES_DIR, \"sample1.jpg\"),\n",
    "    os.path.join(SAMPLES_DIR, \"sample2.jpg\"),\n",
    "    os.path.join(SAMPLES_DIR, \"sample3.jpg\"),\n",
    "]\n",
    "\n",
    "# Verify samples exist\n",
    "for img_path in sample_images:\n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"Found: {img_path}\")\n",
    "    else:\n",
    "        print(f\"Missing: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Sample 1\n",
    "result1 = pipeline.process(sample_images[0], generate_redacted=True)\n",
    "print_pii_report(result1)\n",
    "visualize_results(result1)\n",
    "save_results(result1, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Sample 2\n",
    "result2 = pipeline.process(sample_images[1], generate_redacted=True)\n",
    "print_pii_report(result2)\n",
    "visualize_results(result2)\n",
    "save_results(result2, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Sample 3\n",
    "result3 = pipeline.process(sample_images[2], generate_redacted=True)\n",
    "print_pii_report(result3)\n",
    "visualize_results(result3)\n",
    "save_results(result3, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "all_results = [result1, result2, result3]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF ALL PROCESSED DOCUMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"\\n--- Document {i}: {os.path.basename(result['input_file'])} ---\")\n",
    "    \n",
    "    # Count PII by type\n",
    "    if 'pii_summary' in result:\n",
    "        total_pii = sum(len(v) for v in result['pii_summary'].values())\n",
    "        print(f\"Total PII Found: {total_pii}\")\n",
    "        \n",
    "        for pii_type, values in result['pii_summary'].items():\n",
    "            print(f\"  {pii_type}: {len(values)} items\")\n",
    "            for v in values[:3]:  # Show first 3\n",
    "                print(f\"    - {v}\")\n",
    "            if len(values) > 3:\n",
    "                print(f\"    ... and {len(values)-3} more\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Results saved to: {OUTPUT_DIR}/\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pipeline Usage Function (For New Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_document(image_path: str, output_dir: str = \"results\"):\n",
    "    \"\"\"\n",
    "    Process a new document through the OCR PII pipeline.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the handwritten document image (JPEG)\n",
    "        output_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all extraction results\n",
    "    \"\"\"\n",
    "    # Initialize pipeline with default config\n",
    "    pipeline = OCRPIIPipeline(PipelineConfig())\n",
    "    \n",
    "    # Process the document\n",
    "    result = pipeline.process(image_path, generate_redacted=True)\n",
    "    \n",
    "    # Print report\n",
    "    print_pii_report(result)\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_results(result)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(result, output_dir)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# result = process_new_document(\"path/to/your/document.jpg\")\n",
    "print(\"Pipeline ready for processing new documents!\")\n",
    "print(\"Usage: result = process_new_document('path/to/document.jpg')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}